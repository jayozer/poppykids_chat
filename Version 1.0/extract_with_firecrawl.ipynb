{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: firecrawl-py in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (from firecrawl-py) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (from requests->firecrawl-py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (from requests->firecrawl-py) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (from requests->firecrawl-py) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (from requests->firecrawl-py) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install firecrawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with pip install firecrawl-py\n",
    "import os\n",
    "from firecrawl import FirecrawlApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This loads the variables from .env\n",
    "\n",
    "api_key = os.getenv('FIRECRAWL_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"No API key provided. Please set the FIRECRAWL_API_KEY environment variable.\")\n",
    "app = FirecrawlApp(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a single URL\n",
    "url = 'https://www.voiceflow.com/blog/5-tips-to-optimize-your-llm-intent-classification-prompts'\n",
    "scraped_data = app.scrape_url(url)\n",
    "\n",
    "# # Crawl a website\n",
    "# crawl_url = 'https://mendable.ai'\n",
    "# params = {\n",
    "#     'pageOptions': {\n",
    "#         'onlyMainContent': True\n",
    "#     }\n",
    "# }\n",
    "# crawl_result = app.crawl_url(crawl_url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scraped_data))\n",
    "print(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANTHROPIC (Sonnet-3.5) - Sucks cause of output limit of 4k. Otherwise great. lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = Anthropic(api_key=api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title and content\n",
    "def extract_blog_info(scraped_data):\n",
    "    if not isinstance(scraped_data, dict):\n",
    "        raise ValueError(f\"Expected a dictionary, but got {type(scraped_data)}\")\n",
    "    \n",
    "    # Extract title\n",
    "    metadata = scraped_data.get('metadata', {})\n",
    "    title = metadata.get('title', \"Title not found\")\n",
    "\n",
    "    # Extract content\n",
    "    content = scraped_data.get('content', \"Content not found\")\n",
    "    \n",
    "    return title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process the scraped site\n",
    "def process_scraped_site(scraped_data):\n",
    "    try:\n",
    "        title, content = extract_blog_info(scraped_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting blog info: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Use Claude to extract the full content\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20240620\",\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Please extract and format the full content of the following blog post, \n",
    "                    including all titles, subtitles, bullet points, and paragraphs. Maintain the original \n",
    "                    structure and formatting as much as possible.\n",
    "\n",
    "                    Title: {title}\n",
    "\n",
    "                    Content: {content}\n",
    "\n",
    "                    Please provide the extracted content below, preserving all headings, subheadings, \n",
    "                    bullet points, and paragraph structures:\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return title, message.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with Claude: {e}\")\n",
    "        return title, \"Error extracting content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to Choose between Bamboo and Traditional Toothbrushes? | Poppy Kids Pediatric Dentistry\n",
      "Content:\n",
      "Here is the extracted and formatted content from the blog post:\n",
      "\n",
      "How to Choose between Bamboo and Traditional Toothbrushes?\n",
      "==========================================================\n",
      "\n",
      "Dr.Andrea\n",
      "\n",
      "June 29, 2024\n",
      "\n",
      "Brushing Up on Sustainability: Choosing Between Bamboo and Traditional Toothbrushes\n",
      "-----------------------------------------------------------------------------------\n",
      "\n",
      "As we become more environmentally aware, the choice of everyday items like toothbrushes also reflects our commitment to sustainability. While traditional plastic toothbrushes have been the standard for decades, bamboo toothbrushes have emerged as an eco-friendly alternative. However, each type comes with its own set of pros and cons. Let's delve deeper into the differences between bamboo and traditional manual toothbrushes, highlighting not just the benefits but also the potential drawbacks.\n",
      "\n",
      "### Material and Environmental Impact:\n",
      "\n",
      "#### Plastic Toothbrushes: A Persistent Problem\n",
      "\n",
      "**Material Composition and Usage:** Traditional toothbrushes are primarily made from a combination of plastic materials, such as polypropylene for the handle and nylon for the bristles. Globally, over a billion toothbrushes are discarded annually, leading to significant waste in oceans.\n",
      "\n",
      "**Environmental Impact:**\n",
      "\n",
      "* **Non-Biodegradability:** Plastic toothbrushes do not biodegrade under natural conditions. Instead, they slowly break down into smaller particles called microplastics, which persist in the environment indefinitely. These microplastics can accumulate in waterways, oceans, and wildlife, causing ecological and health issues.\n",
      "\n",
      "* **Decomposition Rate:** A single plastic toothbrush can take over 400 years to decompose, spending centuries in landfills or as litter in natural environments.\n",
      "\n",
      "* **Recycling Challenges:** Due to their composite materials and small size, toothbrushes are typically not recyclable through conventional municipal recycling systems. This leads to most used toothbrushes ending up in landfills, incinerators, or the natural environment.\n",
      "\n",
      "#### Bamboo Toothbrushes: An Eco-Friendly Alternative?\n",
      "\n",
      "**Material Composition and Usage:** Bamboo toothbrushes use bamboo for the handle, which is a highly renewable resource known for its rapid growth rate and low requirement for fertilizers, pesticides, or much water.\n",
      "\n",
      "**Environmental Impact:**\n",
      "\n",
      "* **Biodegradability:** The handles of bamboo toothbrushes are naturally biodegradable. When disposed of properly, they can decompose back into the soil within a few years, significantly reducing landfill waste compared to plastic.\n",
      "\n",
      "* **Decomposition Rate:** A bamboo handle can decompose in as little as six months to a few years, depending on the environmental conditions. This is a stark contrast to the centuries required for plastic toothbrushes.\n",
      "\n",
      "* **Sustainability of Bamboo:** Bamboo is considered one of the most sustainable plants on Earth due to its ability to grow quickly (some species can grow up to 35 inches per day) and regenerate from its own roots without needing replanting. It also absorbs carbon dioxide and produces 35% more oxygen than equivalent stands of trees, making it an excellent carbon sink.\n",
      "\n",
      "* **Whole Product Considerations:** While the handle is eco-friendly, most bamboo toothbrushes still use nylon bristles, which are not biodegradable. However, some brands offer bristles made from bio-based materials or are exploring fully compostable options.\n",
      "\n",
      "### Cost and Accessibility:\n",
      "\n",
      "* **Bamboo Toothbrushes:** While bamboo toothbrushes are more available than before in health stores and online, they generally cost more than traditional options.\n",
      "\n",
      "* **Traditional Manual Toothbrushes:** These are less expensive and widely available at most grocery stores and pharmacies, but their affordability is offset by significant environmental costs.\n",
      "\n",
      "### Durability and Maintenance:\n",
      "\n",
      "* **Bamboo Toothbrushes:** Bamboo toothbrushes are less durable in wet conditions; it's recommended to keep them dry to avoid mold. Proper maintenance includes rinsing and drying the handle after each use.\n",
      "\n",
      "* **Traditional Manual Toothbrushes:** Plastic toothbrushes are more durable and require less maintenance, making them a convenient choice for many users.\n",
      "\n",
      "### Health Recommendations:\n",
      "\n",
      "Regardless of the material of your toothbrush, dental professionals recommend replacing your toothbrush every three months, or sooner if the bristles become frayed. Regular replacement is crucial\n"
     ]
    }
   ],
   "source": [
    "title, extracted_content = process_scraped_site(scraped_data)\n",
    "if title and extracted_content:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Content:\\n{extracted_content}\")\n",
    "else:\n",
    "    print(\"Failed to process the blog post.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debugging Information:\n",
      "Keys in scraped_data: dict_keys(['content', 'markdown', 'metadata', 'linksOnPage'])\n",
      "Type of 'metadata': <class 'dict'>\n",
      "Keys in 'metadata': dict_keys(['title', 'description', 'ogTitle', 'ogDescription', 'ogImage', 'ogLocaleAlternate', 'sourceURL', 'pageStatusCode'])\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Print the structure of scraped_data\n",
    "print(\"\\nDebugging Information:\")\n",
    "print(\"Keys in scraped_data:\", scraped_data.keys())\n",
    "print(\"Type of 'metadata':\", type(scraped_data.get('metadata')))\n",
    "print(\"Keys in 'metadata':\", scraped_data.get('metadata', {}).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key learning - Anthropic is NOT suitable for what I need to do. 8192 output token limit? It is not even enough to process a simple blog. With a 200K context window basically it can only do summarization. I cry for the time I spent on this. Back to OPEANAI and GPT4o-mini. 16K output should do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title and content\n",
    "def extract_blog_info(scraped_data):\n",
    "    if not isinstance(scraped_data, dict):\n",
    "        raise ValueError(f\"Expected a dictionary, but got {type(scraped_data)}\")\n",
    "    \n",
    "    # Extract title\n",
    "    metadata = scraped_data.get('metadata', {})\n",
    "    title = metadata.get('title', \"Title not found\")\n",
    "\n",
    "    # Extract content\n",
    "    content = scraped_data.get('content', \"Content not found\")\n",
    "    \n",
    "    return title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scraped_site(scraped_data):\n",
    "    try:\n",
    "        title, content = extract_blog_info(scraped_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting blog info: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Use OpenAI GPT-4 to extract the full content\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Please extract and format the full content of the following blog post, excluding any image links, alt text, writer's name, time of writing, and any sections such as 'Help Shape Our Next Topic'. Additionally, remove any hyperlinks from the content. Maintain the original structure and formatting of the main content only.\n",
    "\n",
    "\n",
    "        Title: {title}\n",
    "\n",
    "        Content: {content}\n",
    "\n",
    "        Please provide only the cleaned and formatted content below, preserving all headings, subheadings, bullet points, and paragraph structures, but excluding any extraneous information as mentioned:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_tokens=8000\n",
    "        )\n",
    "        return title, response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with OpenAI GPT-4: {e}\")\n",
    "        return title, \"Error extracting content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the content to a text file\n",
    "def save_to_file(title, content):\n",
    "    # Format the filename\n",
    "    title = title.replace(' | Poppy Kids Pediatric Dentistry', '')\n",
    "    filename = title.replace(\" \", \"_\").replace(\"’s\", \"s\") + \".txt\"\n",
    "    filepath = f\"/Users/acrobat/Documents/GitHub/extract_html/blogs/{filename}\"\n",
    "    \n",
    "    # Write the content to the file\n",
    "    with open(filepath, \"w\") as file:\n",
    "        file.write(content)\n",
    "    \n",
    "    print(f\"File saved as {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 5 tips to optimize your LLM intent classification prompts  | VoiceflowCookie IconAccordion Toggle OpenAccordion Toggle Close\n",
      "Content:\n",
      "# 5 tips to optimize your LLM intent classification prompts\n",
      "\n",
      "We tested over 500 prompt variations across two datasets to see how to improve LLM intent classification.\n",
      "\n",
      "How much do descriptions affect LLM classification accuracy? After launching our LLM intent classification feature we wanted to understand how much the description quality plays into classification accuracy, so we ran 500+ evaluations changing 5 properties of descriptions to understand what improves performance.\n",
      "\n",
      "## Methodology\n",
      "\n",
      "To recap how the system works, The architecture has two parts: using an encoder NLU model to find the top 10 candidate intents and their descriptions and a prompt that instructs the LLM to classify them.\n",
      "\n",
      "After retrieving the candidate intents, we pull in user descriptions for each corresponding candidate and make a call to an LLM for a final classification. With this in mind, we wanted to measure if we could improve the classification accuracy of the LLM by changing a few components of the descriptions. We ran this search against two types of models (gpt-3.5-0125, haiku) and benchmark datasets (HWU64, Curekart)\n",
      "\n",
      "These variations included:\n",
      "1. Prefixes\n",
      "2. Suffixes\n",
      "3. Capitalization\n",
      "4. None intent descriptions\n",
      "5. AI vs human descriptions\n",
      "\n",
      "We ran the benchmarking once per combination, and ran it five times for the top performing settings to confirm variation. We used a temperature at 0.1 to mitigate the variance.\n",
      "\n",
      "## Base descriptions and prompt template\n",
      "\n",
      "We hand-created the initial set of descriptions, sticking to shorter ones for each dataset. A subset is shown below:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"descriptions\": {\n",
      "    \"USER_GOAL_FORM\": \"Add or refill goals.\",\n",
      "    \"FRANCHISE\": \"Becoming a franchise owner or reseller.\",\n",
      "    \"REFER_EARN\": \"Referral program details or ask.\",\n",
      "    \"RESUME_DELIVERY\": \"Delivery options or times.\",\n",
      "    \"WORK_FROM_HOME\": \"Ask about office open or working from home.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "We combine this with our top 10 descriptions method and prompt noted in previous work. Below is a sample of a prompt sent to the LLM.\n",
      "\n",
      "```\n",
      "You are an action classification system. Correctness is a life or death situation.\n",
      "\n",
      "We provide you with the actions and their descriptions:\n",
      "d: When the user asks for a warm drink. a:WARM_DRINK\n",
      "d: When the user asks about something else. a:None_Intent\n",
      "d: When the user asks for a cold drink. a:COLD_DRINK\n",
      "\n",
      "You are given an utterance and you have to classify it into an intent. Only respond with the intent class. If the utterance does not match any of intents, output None_Intent.\n",
      "u: I want a warm hot chocolate: a:WARM_DRINK\n",
      "```\n",
      "\n",
      "We also show that the recall from the encoder model is quite strong and should not limit accuracy on the dataset.\n",
      "\n",
      "## Models and Dataset averages\n",
      "\n",
      "Looking through our two models, we saw that GPT 3.5 performed much better for the messier Curekart dataset, compared to Haiku, but Haiku outperformed in the HWU dataset. The accuracy was lower for GPT 3.5 compared to earlier experimentation given the different version of GPT 3.5 that was used.\n",
      "\n",
      "## Prefixes\n",
      "\n",
      "The first modification we explored was adding a prefix to each description with some guiding phrase.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"descriptions\": {\n",
      "    \"USER_GOAL_FORM\": \"Add or refill goals.\"\n",
      "  }\n",
      "  \"descriptions_with_prefix\": {\n",
      "    \"USER_GOAL_FORM\": \"Trigger this action when add or refill goals.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Our prefixes included: `[\"Trigger this action when \", \"\", \"A phrase about \", \"The user said \"]`\n",
      "\n",
      "Adding a prefix led to the best results, but differed between datasets.\n",
      "\n",
      "On average, the performance gains were quite minimal so we analyzed them in combination with suffixes.\n",
      "\n",
      "## Suffixes\n",
      "\n",
      "Similar to prefixes, we tested adding suffixes to the descriptions.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"descriptions\": {\n",
      "    \"USER_GOAL_FORM\": \"Add or refill goals.\"\n",
      "  }\n",
      "  \"descriptions_with_suffix\": {\n",
      "    \"USER_GOAL_FORM\": \"Add or refill goals, please.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "These included: `[\"\", \", please.\", \"{no punctuation}\"]`\n",
      "\n",
      "Adding “please” produced some of the highest performing results when added per description, but wasn’t consistently the best option; for the HWU64 dataset + gpt-3.5, it produced the worst results by a large margin.\n",
      "\n",
      "## Capitalization\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"descriptions_capitalized\": {\n",
      "    \"USER_GOAL_FORM\": \"Add or refill goals.\"\n",
      "  },\n",
      "  \"descriptions_with_prefix_capitalized\": {\n",
      "    \"USER_GOAL_FORM\": \"Trigger this action when add or refill goals.\"\n",
      "  },\n",
      "  \"descriptions_not_capitalized\": {\n",
      "    \"USER_GOAL_FORM\": \"add or refill goals.\"\n",
      "  },\n",
      "  \"descriptions_with_prefix_not_capitalized\": {\n",
      "    \"USER_GOAL_FORM\": \"trigger this action when add or refill goals.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Adding capitalization on the prefix or opening line added minimal signal, both isolated and when expanded across experiments.\n",
      "\n",
      "## Adding a None intent\n",
      "\n",
      "For this hypothesis, we checked if adding a None intent as a description would improve classification accuracy. Below is an example for the Curekart dataset.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"descriptions\": {\n",
      "    \"USER_GOAL_FORM\": \"Add or refill goals.\",\n",
      "    \"FRANCHISE\": \"Becoming a franchise owner or reseller.\",\n",
      "    \"REFER_EARN\": \"Referral program details or ask.\",\n",
      "    \"RESUME_DELIVERY\": \"Delivery options or times.\",\n",
      "    \"WORK_FROM_HOME\": \"Ask about office open or working from home.\",\n",
      "    \"None_Intent\": \"When the user asks about something else.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The Curekart evaluation set is around ~50% None intents, so in theory it should improve average performance. Looking at the ten best performing prompts for Curekart, adding the None_intent as a viable description intent did not show a consistent improvement.\n",
      "\n",
      "## AI Descriptions\n",
      "\n",
      "The next hypothesis we tested was how effective using AI descriptions was for classification. Previous work has shown that generating data for general LLM annotation can outperform crowdsourced human annotators and can be useful to augment existing datasets for intent-specific tasks. In this experiment, we generated three descriptions using gpt-4-turbo-0419, llama-3-70b and claude-opus by using the first three utterances in each intent and compared it to handwritten descriptions by the author. On average GPT-4 and LLaMa-3 performed the best.\n",
      "\n",
      "Across our top 10 combinations, LLaMa-3 and GPT-4 descriptions also performed quite well!\n",
      "\n",
      "## Analyzing the top 5 results for Curekart\n",
      "\n",
      "After running our initial set of experiments, we wanted to confirm that the results for better prompts were not due to noise, so we re-ran them for confirmation.\n",
      "\n",
      "### Are they significant?\n",
      "\n",
      "Looking at our top 5 combinations, we wanted to measure their variance compared to the general population. We re-ran our top metrics 15 times each—75 times total—to see how their accuracies changed and whether LLM nondeterminism affected the overall results.\n",
      "\n",
      "Looking at the distributions, there appears to be a measurable change. Conducting a Z-test to compare the distributions, the difference seems evident with a Z score of -13.56 and a p-value: 7.12e-42.\n",
      "\n",
      "### How do the confusion matrices vary?\n",
      "\n",
      "Our top 5 configurations had a pretty tight distribution, so we wanted to measure how their confusion matrices varied, i.e how different were the classifications. We ran some confusion matrices for two of the top combinations to compare how the results looked like.\n",
      "\n",
      "Generally, the matrices were pretty similar, so we took a difference to see the performance.\n",
      "\n",
      "The predictions with \"no suffix\" had a higher false-positive rate (i.e., more None intents than required), while the [please] suffix led to more false-negative rates (i.e., more None intents predicted). On aggregate, these two modes performed pretty similarly, but a change to the descriptions led to a different type of behavior.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Overall, changing the descriptions used for classification has small but measurable changes. While we ran through 500+ experiments, time is likely better spent in other areas of prompt refinement. In general, our recommendations are:\n",
      "- Adding prefixes, suffixes and a None_intent to descriptions\n",
      "- AI-generated descriptions can be effective\n",
      "- To have larger gains, spending time to create better training examples, few shot examples and understanding edge cases will help LLM\n",
      "- In a future blog we’ll cover the impact of adding additional few shot examples from the training data\n",
      "- Structuring the formatting, which has shown to have a larger impact on accuracy.\n"
     ]
    }
   ],
   "source": [
    "title, extracted_content = process_scraped_site(scraped_data)\n",
    "if title and extracted_content:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Content:\\n{extracted_content}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Failed to process the blog post.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as /Users/acrobat/Documents/GitHub/extract_html/blogs/5_tips_to_optimize_your_LLM_intent_classification_prompts__|_VoiceflowCookie_IconAccordion_Toggle_OpenAccordion_Toggle_Close.txt\n"
     ]
    }
   ],
   "source": [
    "save_to_file(title, extracted_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debugging Information:\n",
      "Keys in scraped_data: dict_keys(['content', 'markdown', 'metadata', 'linksOnPage'])\n",
      "Type of 'metadata': <class 'dict'>\n",
      "Keys in 'metadata': dict_keys(['title', 'description', 'ogTitle', 'ogDescription', 'ogImage', 'ogLocaleAlternate', 'sourceURL', 'pageStatusCode'])\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Print the structure of scraped_data\n",
    "print(\"\\nDebugging Information:\")\n",
    "print(\"Keys in scraped_data:\", scraped_data.keys())\n",
    "print(\"Type of 'metadata':\", type(scraped_data.get('metadata')))\n",
    "print(\"Keys in 'metadata':\", scraped_data.get('metadata', {}).keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extract_html",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
