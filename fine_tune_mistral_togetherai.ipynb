{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create json file to upload to together AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format to the one that the chatbot can understand\n",
    "\"\"\"Question: How should parents maintain the oral hygiene of a baby with a cleft palate or lip?\n",
    "Answer: It's crucial to wipe the baby's mouth with a soft, clean cloth after feeding, especially during the first few months when saliva production is low, and consult with dental professionals to devise an effective oral care plan.\n",
    "Q: When should a child with HFMD see a doctor?\n",
    "A: Consult a doctor if the child is unable to drink enough liquids, shows signs of dehydration, or if symptoms worsen after a few days.\n",
    "\n",
    "Q: What should a parent do if their infant's diarrhea persists?\n",
    "A: If an infant's diarrhea persists, parents should consult a pediatrician. Immediate medical attention is advised if the infant experiences severe diarrhea for one day, mild to moderate diarrhea for more than two or three days, vomiting, refusal to eat or drink, severe stomach pain, bloody diarrhea, or a high fever.\n",
    "\n",
    "Q: What are some safe and recommended teething relief options for infants?\n",
    "A: Safe teething relief options include cold foods like refrigerated applesauce or yogurt, chilled teething rings or toys, frozen cloths (thawed before use), and teething biscuits for children who are eight to 12 months old.\n",
    "\n",
    "Q: What are some effective ways to relieve the discomfort caused by herpangina?\n",
    "A: To relieve discomfort from herpangina, you can give your child pain and fever reducers, encourage the intake of cold fluids or sugar-free ice pops, provide a bland diet, and have them gargle with cool water. Maintaining oral hygiene, even when sick, is also crucial.\n",
    "\n",
    "Q: Is herpangina contagious and what precautions should be taken?\n",
    "A: Yes, herpangina is highly contagious. It's important to practice good hand hygiene, disinfect surfaces that the infected child has touched, and keep the child isolated at home, away from school or daycare, until they have fully recovered to prevent spreading the virus to others.\n",
    "\n",
    "Q: When should a child with herpangina see a dentist?\n",
    "A: While herpangina mainly requires medical diagnosis and care, consult a dentist if mouth sores severely impact the child's oral hygiene or if advice is needed on suitable oral care products during illness, such as age-appropriate mouthwash to soothe mouth irritations.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "{\"text\": \"<user>: How should parents maintain the oral hygiene of a baby with a cleft palate or lip? <bot> : It's crucial to wipe the baby's mouth with a soft, clean cloth after feeding, especially during the first few months when saliva production is low, and consult with dental professionals to devise an effective oral care plan.\"}\n",
    "{\"text\": \"<user>: When should a child with HFMD see a doctor? <bot> : Consult a doctor if the child is unable to drink enough liquids, shows signs of dehydration, or if symptoms worsen after a few days.\"}\n",
    "{\"text\": \"<user>: What should a parent do if their infant's diarrhea persists? <bot> : If an infant's diarrhea persists, parents should consult a pediatrician. Immediate medical attention is advised if the infant experiences severe diarrhea for one day, mild to moderate diarrhea for more than two or three days, vomiting, refusal to eat or drink, severe stomach pain, bloody diarrhea, or a high fever.\"}\n",
    "{\"text\": \"<user>: What are some safe and recommended teething relief options for infants? <bot> : Safe teething relief options include cold foods like refrigerated applesauce or yogurt, chilled teething rings or toys, frozen cloths (thawed before use), and teething biscuits for children who are eight to 12 months old.\"}\n",
    "{\"text\": \"<user>: What are some effective ways to relieve the discomfort caused by herpangina? <bot> : To relieve discomfort from herpangina, you can give your child pain and fever reducers, encourage the intake of cold fluids or sugar-free ice pops, provide a bland diet, and have them gargle with cool water. Maintaining oral hygiene, even when sick, is also crucial.\"}\n",
    "{\"text\": \"<user>: Is herpangina contagious and what precautions should be taken? <bot> : Yes, herpangina is highly contagious. It's important to practice good hand hygiene, disinfect surfaces that the infected child has touched, and keep the child isolated at home, away from school or daycare, until they have fully recovered to prevent spreading the virus to others.\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"<user>: Can the use of pacifiers lead to dental problems in children? <bot> : Yes, excessive use of pacifiers can cause abnormal tooth development, affect the growth of the mouth, and misalign the teeth. It can also alter the shape of the roof of the mouth if used extensively, particularly in children older than two years.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What are the benefits of using a pacifier for infants? <bot> : Pacifiers can soothe a crying infant, reduce pain during nursing, decrease the risk of sudden infant death syndrome (SIDS), and are particularly beneficial for premature infants in intensive care, helping shorten hospital stays and aiding in bottle feeding.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: Are there any risks associated with pacifier use beyond dental issues? <bot> : Beyond dental issues, pacifiers can increase the risk of ear infections and can interfere with breastfeeding. It is recommended to introduce a pacifier only after the baby is comfortably breastfeeding.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: How can parents minimize the dental risks associated with pacifiers? <bot> : Parents can minimize dental risks by monitoring the intensity of sucking\\u2014passive placement in the mouth is less harmful than vigorous sucking. It is also important to use one-piece pacifiers without removable parts or liquid interiors, keep them clean, and avoid dipping them in sweet liquids.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: When should a child ideally stop using a pacifier to avoid dental issues? <bot> : Children should ideally stop using a pacifier between the ages of two and four, before the permanent teeth are ready to erupt, to avoid long-term dental issues.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What are baby teeth wipes and how do they function? <bot> : Baby teeth wipes are small, gauze-like pads saturated with xylitol solution, designed to prevent tooth decay by reducing harmful oral bacteria, particularly Streptococcus mutans.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What is the role of xylitol in baby teeth wipes? <bot> : Xylitol is a five-carbon sugar alcohol that hinders cavity-causing bacteria from processing it effectively. It is used in baby teeth wipes to help reduce the risk of early childhood cavities, although further research on its efficacy is needed.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: Are baby teeth wipes effective in reducing cavity-causing bacteria? <bot> : Yes, studies, such as one published in the International Journal of Clinical Pediatric Dentistry, have shown that using xylitol-saturated teeth wipes can significantly decrease the levels of cavity-causing bacteria in children's mouths.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: How should baby teeth wipes be used? <bot> : Baby teeth wipes should be used to wipe the child's teeth and gums twice a day, following the instructions provided on the product packaging or as advised by your dentist.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: Should parents consult a dentist before using baby teeth wipes? <bot> : Yes, it is advisable for parents to consult their dentist before introducing baby teeth wipes or any new dental products to ensure they are suitable and safe for their child's oral care routine.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: Why is it important for children to brush their teeth regularly? <bot> : Regular brushing is crucial for children to prevent tooth decay, discomfort, and issues with future permanent teeth. Establishing good dental hygiene early helps avoid chronic conditions like cavities, which affect about 20% of children between 5 and 11 years old.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: How can parents help their child learn proper tooth brushing techniques? <bot> : Parents can educate their children on proper brushing through fun books, videos, and play, using dental activities like drawing and coloring teeth on a poster board. Using a timer can ensure children brush for the recommended two minutes, and regular dental visits can reinforce proper techniques.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What are some effective ways to make tooth brushing fun for children? <bot> : To make tooth brushing enjoyable, parents can brush their teeth alongside their children, sing songs, play games, and let children choose their own toothbrushes and toothpaste. Incentives like notes from the tooth fairy and reward charts can also motivate children to brush their teeth.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What should parents do if their child still refuses to brush their teeth? <bot> : If a child persistently refuses to brush, it could be due to sensitive teeth or other discomforts. Parents should consult a dental professional to address any pain and explore suitable products or practices to alleviate it. Continuing to model good oral hygiene and making it fun can also encourage reluctant children.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What can parents do to effectively integrate tooth brushing into their toddler's daily routine? <bot> : Parents can achieve this by making tooth brushing a fun and engaging activity, and by consistently incorporating it into the toddler's daily routine while offering positive reinforcement.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: What approach is suggested for parents to follow when teaching toddlers how to brush their teeth? <bot> : Parents are advised to let toddlers take the lead in brushing their teeth while they provide guidance and express pride in the toddler's efforts to encourage continued interest and improvement in brushing skills.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: How can Baby Bottle Tooth Decay be prevented? <bot> : Prevention methods include wiping the baby's gums with a damp cloth after feeding, avoiding saliva sharing, using a grain of rice-sized amount of fluoride toothpaste for brushing baby teeth, and avoiding sugary drinks in bottles.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: How can parents ensure their child develops good oral hygiene habits? <bot> : Parents can ensure good oral hygiene by cleaning their baby's mouth after each feeding, supervising and assisting older children with brushing using fluoride toothpaste, and instilling healthy dietary habits.\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"<user>: When is the right time to start using fluoridated toothpaste for a child, and why? <bot> : Parents should start using fluoridated toothpaste when the child is at least 2 years old to avoid fluorosis, which can cause tooth discoloration and damage.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_qa_to_json(filepath):\n",
    "    # Read the file content\n",
    "    with open(filepath, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content into individual Q&A pairs based on the Q: and A: tags\n",
    "    questions_answers = content.split('Q: ')[1:]  # Ignore the first split as it will be empty\n",
    "\n",
    "    # List to hold the formatted Q&A pairs\n",
    "    formatted_pairs = []\n",
    "\n",
    "    # Process each Q&A pair\n",
    "    for qa in questions_answers:\n",
    "        # Split each part by the A: tag, ensuring we only split at the first occurrence\n",
    "        parts = qa.split('\\nA: ')\n",
    "        question = parts[0].strip()\n",
    "        answer = parts[1].strip()\n",
    "\n",
    "        # Construct the JSON-like dictionary\n",
    "        json_dict = {\n",
    "            \"text\": f\"<user>: {question} <bot> : {answer}\"\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        formatted_pairs.append(json_dict)\n",
    "\n",
    "    return formatted_pairs\n",
    "\n",
    "# Example usage of the function\n",
    "filepath = '/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/topics/Oral_Hygiene_Practices.txt'  \n",
    "converted_data = convert_qa_to_json(filepath)\n",
    "print(json.dumps(converted_data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of json.dump I want to write the output to a file in .jsonl format\n",
    "import json\n",
    "\n",
    "def convert_qa_to_json(filepath, output_filepath):\n",
    "    # Read the file content\n",
    "    with open(filepath, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content into individual Q&A pairs based on the Question: and Answer: tags\n",
    "    questions_answers = content.split('Question: ')[1:]  # Ignore the first split as it will be empty\n",
    "\n",
    "    # Open the output file in write mode\n",
    "    with open(output_filepath, 'w') as out_file:\n",
    "        # Process each Q&A pair\n",
    "        for qa in questions_answers:\n",
    "            # Split each part by the Answer: tag, ensuring we only split at the first occurrence\n",
    "            parts = qa.split('\\nAnswer: ')\n",
    "            question = parts[0].strip()\n",
    "            answer = parts[1].strip()\n",
    "\n",
    "            # Construct the JSON-like dictionary\n",
    "            json_dict = {\n",
    "                \"text\": f\"<user>: {question} <bot> : {answer}\"\n",
    "            }\n",
    "\n",
    "            # Write the dictionary to the file as a JSON line\n",
    "            out_file.write(json.dumps(json_dict) + '\\n')  # Add a newline to separate entries\n",
    "\n",
    "# Example usage of the function\n",
    "filepath = '/Users/acrobat/Documents/GitHub/extract_html/clean_faq_dataset.txt'  # Actual file path\n",
    "output_filepath = '/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_dataset.jsonl'  # The output file name and path\n",
    "convert_qa_to_json(filepath, output_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally I know the number of questions I have 844!!! I do think this is good enough to fine tune a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Together AI set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Claude API key from the environment variables\n",
    "together_api_key = os.getenv(\"TOGETHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "#print(wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_check_passed': True, 'message': 'Checks passed', 'found': True, 'file_size': 250867, 'utf8': True, 'line_type': True, 'text_field': True, 'key_value': True, 'min_samples': True, 'num_samples': 844, 'load_json': True, 'filetype': 'jsonl'}\n"
     ]
    }
   ],
   "source": [
    "import together\n",
    "import os\n",
    "\n",
    "together.api_key = together_api_key\n",
    "\n",
    "resp = together.Files.check(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_dataset.jsonl\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are go ladies and gentlemen. We have our dataset ready. 844 samples. Let's train. Starting at 5pm Sunday. Lets see how long it takes!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/5w1dty5j7nx2sk_l039y73900000gn/T/ipykernel_87035/901815337.py:1: DeprecationWarning: Call to deprecated function upload.\n",
      "  resp = together.Files.upload(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_dataset.jsonl\")\n",
      "/Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages/together/legacy/files.py:44: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n",
      "Uploading file togetherai_faq_dataset.jsonl: 100%|██████████| 251k/251k [00:00<00:00, 253kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-957fe0f3-1396-4642-9f51-899e2897a9f8\n"
     ]
    }
   ],
   "source": [
    "resp = together.Files.upload(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_dataset.jsonl\")\n",
    "file_id = resp[\"id\"]\n",
    "print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = 'file-957fe0f3-1396-4642-9f51-899e2897a9f8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_check_passed': True, 'message': 'Checks passed', 'found': True, 'file_size': 250023, 'utf8': True, 'line_type': True, 'text_field': True, 'key_value': True, 'min_samples': True, 'num_samples': 844, 'load_json': True, 'filetype': 'jsonl'}\n"
     ]
    }
   ],
   "source": [
    "import together\n",
    "import os\n",
    "\n",
    "together.api_key = together_api_key\n",
    "\n",
    "resp = together.Files.check(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_datasetv2.jsonl\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 file id\n",
    "resp = together.Files.upload(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_datasetv2.jsonl\")\n",
    "file_id = resp[\"id\"]\n",
    "print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/5w1dty5j7nx2sk_l039y73900000gn/T/ipykernel_87035/2685264592.py:7: DeprecationWarning: Call to deprecated function create.\n",
      "  resp = together.Finetune.create(\n",
      "/Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages/together/legacy/finetune.py:30: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'ft-fb8cf710-4c53-4cc7-aa41-7a4c3326b514', 'training_file': 'file-957fe0f3-1396-4642-9f51-899e2897a9f8', 'validation_file': '', 'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'output_name': 'jayozer@gmail.com/Mistral-7B-Instruct-v0.2-poppychat_finetune-2024-05-06-00-29-01', 'n_epochs': 3, 'n_checkpoints': 1, 'batch_size': 8, 'learning_rate': 1e-05, 'eval_steps': 0, 'lora': False, 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0, 'created_at': '2024-05-06T00:29:01.904Z', 'updated_at': '2024-05-06T00:29:01.904Z', 'status': <FinetuneJobStatus.STATUS_PENDING: 'pending'>, 'job_id': '', 'events': [{'object': <ObjectType.FinetuneEvent: 'fine-tune-event'>, 'created_at': '2024-05-06T00:29:01.904Z', 'level': <FinetuneEventLevels.NULL: ''>, 'message': 'Fine tune request created', 'type': <FinetuneEventType.JOB_PENDING: 'JOB_PENDING'>, 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'hash': '', 'total_steps': 0, 'step': 0, 'checkpoint_path': '', 'model_path': '', 'training_offset': 0}], 'token_count': 0, 'param_count': 0, 'total_price': 0, 'epochs_completed': 0, 'queue_depth': 0, 'wandb_project_name': '', 'wandb_url': '', 'training_file_num_lines': 0, 'training_file_size': 250867, 'model_output_path': 's3://together-dev/finetune/65fa39deacd0355d0a42bf92/jayozer@gmail.com/Mistral-7B-Instruct-v0.2-poppychat_finetune-2024-05-06-00-29-01/ft-fb8cf710-4c53-4cc7-aa41-7a4c3326b514', 'Suffix': 'poppychat_finetune', 'user_id': '65fa39deacd0355d0a42bf92', 'staring_epoch': 0, 'training_offset': 0, 'checkspoint_path': '', 'random_seed': '', 'owner_address': '0xb1adab08ef70d0e0259fb9ce7815a80e07bb429c', 'total_steps': 0, 'steps_completed': 0, 'steps_paid_for': 0, 'wandb_key': '5fd9af8b1ed504e2f4399958efb48a4df2569e3d', 'enable_checkpoints': False, 'checkpoints': [], 'internal_flags': '', 'UsedModelName': '', 'job_stats': {'FtUserTime': '', 'FtSysTime': '', 'FtMaxRss': 0, 'FtMinPgFlt': 0, 'FtMajPgFlt': 0, 'FtInBlock': 0, 'FtOutBlock': 0, 'FtNvCsw': 0, 'FtNivCsw': 0}}\n"
     ]
    }
   ],
   "source": [
    "# run the fine tune job\n",
    "import together\n",
    "import os\n",
    "\n",
    "together.api_key = together_api_key\n",
    "\n",
    "resp = together.Finetune.create(\n",
    "    training_file=file_id,\n",
    "    model = 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    n_epochs=3,\n",
    "    n_checkpoints = 1,\n",
    "    batch_size= 4,\n",
    "    learning_rate= 1e-5,\n",
    "    suffix= 'poppychat_finetune-hermes',\n",
    "    wandb_api_key = wandb_api_key,\n",
    ")\n",
    "\n",
    "fine_tune_id = resp['id']\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/5w1dty5j7nx2sk_l039y73900000gn/T/ipykernel_83424/2160088568.py:7: DeprecationWarning: Call to deprecated function create.\n",
      "  resp = together.Finetune.create(\n",
      "/Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages/together/legacy/finetune.py:30: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'ft-1f9b5ff0-f8df-4c35-81b8-031427ea3e34', 'training_file': 'file-957fe0f3-1396-4642-9f51-899e2897a9f8', 'validation_file': '', 'model': 'teknium/OpenHermes-2p5-Mistral-7B', 'output_name': 'jayozer@gmail.com/OpenHermes-2p5-Mistral-7B-poppychat_finetune-hermes-2024-05-06-13-22-34', 'n_epochs': 3, 'n_checkpoints': 1, 'batch_size': 8, 'learning_rate': 1e-05, 'eval_steps': 0, 'lora': False, 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0, 'created_at': '2024-05-06T13:22:34.623Z', 'updated_at': '2024-05-06T13:22:34.623Z', 'status': <FinetuneJobStatus.STATUS_PENDING: 'pending'>, 'job_id': '', 'events': [{'object': <ObjectType.FinetuneEvent: 'fine-tune-event'>, 'created_at': '2024-05-06T13:22:34.623Z', 'level': <FinetuneEventLevels.NULL: ''>, 'message': 'Fine tune request created', 'type': <FinetuneEventType.JOB_PENDING: 'JOB_PENDING'>, 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'hash': '', 'total_steps': 0, 'step': 0, 'checkpoint_path': '', 'model_path': '', 'training_offset': 0}], 'token_count': 0, 'param_count': 0, 'total_price': 0, 'epochs_completed': 0, 'queue_depth': 0, 'wandb_project_name': '', 'wandb_url': '', 'training_file_num_lines': 0, 'training_file_size': 250867, 'model_output_path': 's3://together-dev/finetune/65fa39deacd0355d0a42bf92/jayozer@gmail.com/OpenHermes-2p5-Mistral-7B-poppychat_finetune-hermes-2024-05-06-13-22-34/ft-1f9b5ff0-f8df-4c35-81b8-031427ea3e34', 'Suffix': 'poppychat_finetune-hermes', 'user_id': '65fa39deacd0355d0a42bf92', 'staring_epoch': 0, 'training_offset': 0, 'checkspoint_path': '', 'random_seed': '', 'owner_address': '0xb1adab08ef70d0e0259fb9ce7815a80e07bb429c', 'total_steps': 0, 'steps_completed': 0, 'steps_paid_for': 0, 'wandb_key': '5fd9af8b1ed504e2f4399958efb48a4df2569e3d', 'enable_checkpoints': False, 'checkpoints': [], 'internal_flags': '', 'UsedModelName': '', 'job_stats': {'FtUserTime': '', 'FtSysTime': '', 'FtMaxRss': 0, 'FtMinPgFlt': 0, 'FtMajPgFlt': 0, 'FtInBlock': 0, 'FtOutBlock': 0, 'FtNvCsw': 0, 'FtNivCsw': 0}}\n"
     ]
    }
   ],
   "source": [
    "# run the fine tune job\n",
    "import together\n",
    "import os\n",
    "\n",
    "together.api_key = together_api_key\n",
    "\n",
    "resp = together.Finetune.create(\n",
    "    training_file=file_id,\n",
    "    model = 'teknium/OpenHermes-2p5-Mistral-7B',\n",
    "    n_epochs=3,\n",
    "    n_checkpoints = 1,\n",
    "    batch_size= 4,\n",
    "    learning_rate= 1e-5,\n",
    "    suffix= 'poppychat_finetune-hermes',\n",
    "    wandb_api_key = wandb_api_key,\n",
    ")\n",
    "\n",
    "fine_tune_id = resp['id']\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate is very slow - The learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function. It defines how quickly a network updates its parameters.\n",
    "\n",
    "In the provided code, the learning rate is set to 1e-5, which is equivalent to 0.00001.\n",
    "\n",
    "This is a relatively small learning rate, which means the model will learn slowly and the weights will be updated in small increments. This can be beneficial because it allows the model to converge to a better solution, but it also means that training might take longer.\n",
    "\n",
    "The optimal learning rate can vary depending on the specific problem and model. It's often chosen through trial and error: you might start with a larger learning rate (like 0.1), and if the model fails to learn, you would try decreasing the learning rate until the model starts to learn effectively.\n",
    "\n",
    "# first run parameters:\n",
    "training_file: This is the ID of the file that contains the training data.\n",
    "model: This is the name of the pre-trained model to be fine-tuned. In this case, it's 'mistralai/Mistral-7B-Instruct-v0.2'.\n",
    "n_epochs: This is the number of epochs for which the model will be trained. An epoch is a complete pass through the entire training dataset.\n",
    "n_checkpoints: This is the number of checkpoints to be saved during training. A checkpoint is a saved state of the model at a particular point in training.\n",
    "batch_size: This is the number of training examples to be used in one iteration of model training.\n",
    "learning_rate: This is the learning rate for the model training. The learning rate determines how much the model's weights are updated in response to the estimated error each time the model weights are updated.\n",
    "suffix: This is a suffix to be added to the name of the fine-tuned model.\n",
    "wandb_api_key: This is the API key for the Weights & Biases service, which is a tool for tracking and visualizing machine learning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States of America is Washington, D.C. (short for District of Columbia).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-8b-chat-hf\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is teh capital of USA?\"}],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! together fine-tuning list-events 'ft-fb8cf710-4c53-4cc7-aa41-7a4c3326b514'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a version 2 data set - Mistral and Open hermes wont do inference trying the same with another dataset and model (Zephr)\n",
    "##### At this point just trying to get inference so chose a smaller model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_check_passed': True, 'message': 'Checks passed', 'found': True, 'file_size': 250023, 'utf8': True, 'line_type': True, 'text_field': True, 'key_value': True, 'min_samples': True, 'num_samples': 844, 'load_json': True, 'filetype': 'jsonl'}\n"
     ]
    }
   ],
   "source": [
    "# file check\n",
    "import together\n",
    "import os\n",
    "\n",
    "together.api_key = together_api_key\n",
    "\n",
    "resp = together.Files.check(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_datasetv2.jsonl\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/5w1dty5j7nx2sk_l039y73900000gn/T/ipykernel_83424/2517792168.py:2: DeprecationWarning: Call to deprecated function upload.\n",
      "  resp = together.Files.upload(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_datasetv2.jsonl\")\n",
      "/Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages/together/legacy/files.py:44: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n",
      "Uploading file togetherai_faq_datasetv2.jsonl: 100%|██████████| 250k/250k [00:00<00:00, 323kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-c5791833-ab62-4565-8f48-536b9fcc6a71\n"
     ]
    }
   ],
   "source": [
    "#Upload to Togetherai and get the file_id for next step\n",
    "resp = together.Files.upload(\"/Users/acrobat/Documents/GitHub/extract_html/data_cleaned_faq_by_topic/togetherai_finetune_faq_dataset/togetherai_faq_datasetv2.jsonl\")\n",
    "file_id = resp[\"id\"]\n",
    "print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'ft-1ac853a8-226b-4248-8af1-864adca500e9', 'training_file': 'file-c5791833-ab62-4565-8f48-536b9fcc6a71', 'validation_file': '', 'model': 'HuggingFaceH4/zephyr-7b-beta', 'output_name': 'jayozer@gmail.com/zephyr-7b-beta-poppychat_finetune-hermes-newfile-2024-05-06-14-26-55', 'n_epochs': 3, 'n_checkpoints': 1, 'batch_size': 8, 'learning_rate': 1e-05, 'eval_steps': 0, 'lora': False, 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0, 'created_at': '2024-05-06T14:26:55.432Z', 'updated_at': '2024-05-06T14:26:55.432Z', 'status': <FinetuneJobStatus.STATUS_PENDING: 'pending'>, 'job_id': '', 'events': [{'object': <ObjectType.FinetuneEvent: 'fine-tune-event'>, 'created_at': '2024-05-06T14:26:55.432Z', 'level': <FinetuneEventLevels.NULL: ''>, 'message': 'Fine tune request created', 'type': <FinetuneEventType.JOB_PENDING: 'JOB_PENDING'>, 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'hash': '', 'total_steps': 0, 'step': 0, 'checkpoint_path': '', 'model_path': '', 'training_offset': 0}], 'token_count': 0, 'param_count': 0, 'total_price': 0, 'epochs_completed': 0, 'queue_depth': 0, 'wandb_project_name': '', 'wandb_url': '', 'training_file_num_lines': 0, 'training_file_size': 250023, 'model_output_path': 's3://together-dev/finetune/65fa39deacd0355d0a42bf92/jayozer@gmail.com/zephyr-7b-beta-poppychat_finetune-hermes-newfile-2024-05-06-14-26-55/ft-1ac853a8-226b-4248-8af1-864adca500e9', 'Suffix': 'poppychat_finetune-hermes-newfile', 'user_id': '65fa39deacd0355d0a42bf92', 'staring_epoch': 0, 'training_offset': 0, 'checkspoint_path': '', 'random_seed': '', 'owner_address': '0xb1adab08ef70d0e0259fb9ce7815a80e07bb429c', 'total_steps': 0, 'steps_completed': 0, 'steps_paid_for': 0, 'wandb_key': '5fd9af8b1ed504e2f4399958efb48a4df2569e3d', 'enable_checkpoints': False, 'checkpoints': [], 'internal_flags': '', 'UsedModelName': '', 'job_stats': {'FtUserTime': '', 'FtSysTime': '', 'FtMaxRss': 0, 'FtMinPgFlt': 0, 'FtMajPgFlt': 0, 'FtInBlock': 0, 'FtOutBlock': 0, 'FtNvCsw': 0, 'FtNivCsw': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/5w1dty5j7nx2sk_l039y73900000gn/T/ipykernel_83424/3714401644.py:7: DeprecationWarning: Call to deprecated function create.\n",
      "  resp = together.Finetune.create(\n",
      "/Users/acrobat/Documents/GitHub/extract_html/extract_html/lib/python3.11/site-packages/together/legacy/finetune.py:30: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# run the fine tune job\n",
    "import together\n",
    "import os\n",
    "\n",
    "together.api_key = together_api_key\n",
    "\n",
    "resp = together.Finetune.create(\n",
    "    training_file='file-c5791833-ab62-4565-8f48-536b9fcc6a71',\n",
    "    model = 'HuggingFaceH4/zephyr-7b-beta',\n",
    "    n_epochs=3,\n",
    "    n_checkpoints = 1,\n",
    "    batch_size= 4,\n",
    "    learning_rate= 1e-5,\n",
    "    suffix= 'poppychat_finetune-hermes-newfile',\n",
    "    wandb_api_key = wandb_api_key,\n",
    ")\n",
    "\n",
    "fine_tune_id = resp['id']\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|    | Message                                     | Type                                              | Created At                 | Hash                 |\n",
      "+====+=============================================+===================================================+============================+======================+\n",
      "|  0 | Fine tune request created                   | FinetuneEventType.JOB_PENDING                     | 2024-05-06 14:26:55.432000 |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  1 | Job started at Mon May  6 07:26:58 PDT 2024 | FinetuneEventType.JOB_START                       | 2024-05-06 14:26:59        | -8421788825774451729 |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  2 | Model data downloaded for                   | FinetuneEventType.MODEL_DOWNLOAD_COMPLETE         | 2024-05-06 14:27:03        | -263667330466734584  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6  |                                                   |                            |                      |\n",
      "|    | 07:27:03 PDT 2024                           |                                                   |                            |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  3 | Training data downloaded for                | FinetuneEventType.TRAINING_DATA_DOWNLOAD_COMPLETE | 2024-05-06 14:27:06        | 8188353049168711722  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6  |                                                   |                            |                      |\n",
      "|    | 07:27:05 PDT 2024                           |                                                   |                            |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  4 | WandB run initialized.                      | FinetuneEventType.WANDB_INIT                      | 2024-05-06 14:27:30        | 522339087551520596   |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  5 | Training started for model                  | FinetuneEventType.TRAINING_START                  | 2024-05-06 14:28:29        | 3068592198844781408  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta                |                                                   |                            |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  6 | Epoch completed, at step 1                  | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:28:43        | 8960916157722346918  |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "! together fine-tuning list-events 'ft-1ac853a8-226b-4248-8af1-864adca500e9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|    | Message                                     | Type                                              | Created At                 | Hash                 |\n",
      "+====+=============================================+===================================================+============================+======================+\n",
      "|  0 | Fine tune request created                   | FinetuneEventType.JOB_PENDING                     | 2024-05-06 14:26:55.432000 |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  1 | Job started at Mon May  6 07:26:58 PDT 2024 | FinetuneEventType.JOB_START                       | 2024-05-06 14:26:59        | -8421788825774451729 |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  2 | Model data downloaded for                   | FinetuneEventType.MODEL_DOWNLOAD_COMPLETE         | 2024-05-06 14:27:03        | -263667330466734584  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6  |                                                   |                            |                      |\n",
      "|    | 07:27:03 PDT 2024                           |                                                   |                            |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  3 | Training data downloaded for                | FinetuneEventType.TRAINING_DATA_DOWNLOAD_COMPLETE | 2024-05-06 14:27:06        | 8188353049168711722  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6  |                                                   |                            |                      |\n",
      "|    | 07:27:05 PDT 2024                           |                                                   |                            |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  4 | WandB run initialized.                      | FinetuneEventType.WANDB_INIT                      | 2024-05-06 14:27:30        | 522339087551520596   |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  5 | Training started for model                  | FinetuneEventType.TRAINING_START                  | 2024-05-06 14:28:29        | 3068592198844781408  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta                |                                                   |                            |                      |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  6 | Epoch completed, at step 1                  | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:28:43        | 8960916157722346918  |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  7 | Epoch completed, at step 2                  | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:28:53        | -6666785510588666905 |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  8 | Epoch completed, at step 3                  | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:29:02        | 8567341567150303272  |\n",
      "+----+---------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "! together fine-tuning list-events 'ft-1ac853a8-226b-4248-8af1-864adca500e9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|    | Message                                      | Type                                              | Created At                 | Hash                 |\n",
      "+====+==============================================+===================================================+============================+======================+\n",
      "|  0 | Fine tune request created                    | FinetuneEventType.JOB_PENDING                     | 2024-05-06 14:26:55.432000 |                      |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  1 | Job started at Mon May  6 07:26:58 PDT 2024  | FinetuneEventType.JOB_START                       | 2024-05-06 14:26:59        | -8421788825774451729 |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  2 | Model data downloaded for                    | FinetuneEventType.MODEL_DOWNLOAD_COMPLETE         | 2024-05-06 14:27:03        | -263667330466734584  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6   |                                                   |                            |                      |\n",
      "|    | 07:27:03 PDT 2024                            |                                                   |                            |                      |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  3 | Training data downloaded for                 | FinetuneEventType.TRAINING_DATA_DOWNLOAD_COMPLETE | 2024-05-06 14:27:06        | 8188353049168711722  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6   |                                                   |                            |                      |\n",
      "|    | 07:27:05 PDT 2024                            |                                                   |                            |                      |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  4 | WandB run initialized.                       | FinetuneEventType.WANDB_INIT                      | 2024-05-06 14:27:30        | 522339087551520596   |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  5 | Training started for model                   | FinetuneEventType.TRAINING_START                  | 2024-05-06 14:28:29        | 3068592198844781408  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta                 |                                                   |                            |                      |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  6 | Epoch completed, at step 1                   | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:28:43        | 8960916157722346918  |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  7 | Epoch completed, at step 2                   | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:28:53        | -6666785510588666905 |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  8 | Epoch completed, at step 3                   | FinetuneEventType.EPOCH_COMPLETE                  | 2024-05-06 14:29:02        | 8567341567150303272  |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "|  9 | Training completed for                       | FinetuneEventType.TRAINING_COMPLETE               | 2024-05-06 14:30:01        | 3899951045498325352  |\n",
      "|    | HuggingFaceH4/zephyr-7b-beta at Mon May  6   |                                                   |                            |                      |\n",
      "|    | 07:30:01 PDT 2024                            |                                                   |                            |                      |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "| 10 | Compressing output model                     | FinetuneEventType.MODEL_COMPRESSING               | 2024-05-06 14:30:03        | -7221212997167522221 |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "| 11 | Model compression complete                   | FinetuneEventType.MODEL_COMPRESSION_COMPLETE      | 2024-05-06 14:30:20        | 808804665367279749   |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "| 12 | Uploading output model                       | FinetuneEventType.MODEL_UPLOADING                 | 2024-05-06 14:30:21        | -6393835333729697209 |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "| 13 | Model upload complete                        | FinetuneEventType.MODEL_UPLOAD_COMPLETE           | 2024-05-06 14:35:49        | -1340577551932170762 |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n",
      "| 14 | Job finished at Mon May  6 07:35:49 PDT 2024 | FinetuneEventType.JOB_COMPLETE                    | 2024-05-06 14:35:49        | 5922460111190967076  |\n",
      "+----+----------------------------------------------+---------------------------------------------------+----------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "! together fine-tuning list-events 'ft-1ac853a8-226b-4248-8af1-864adca500e9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"jayozer@gmail.com/Mistral-7B-Instruct-v0.2-poppychat_finetune-2024-05-06-00-29-01\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of USA?\"}],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I got it working at the end by emailing Togetherai. It turned out to be an issue with Togetherai. The load finetune model times did not change. Still takes about 10 to 15mins for the finetune model to be online. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of my finetuuned Mistral 7B models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image description](images/finetuned_models.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extract_html",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
